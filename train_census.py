# -*- coding: utf-8 -*-
"""Census_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oF1JYGnImRIWV4wdSLXK-4iA8QwEyVXh
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.models import model_from_json
from keras.utils import plot_model
import pydot

def clean(fname):
	myFile = pd.read_csv(fname, sep=',',header=None,na_values=' ?')
	cols = [1,6,13]
	myFile[cols] = myFile[cols].fillna(myFile.mode().iloc[0])
	myFile.replace(' <=50K.',-1,inplace=True)
	myFile.replace(' >50K.',1,inplace=True)
	myFile.replace(' <=50K',-1,inplace=True)
	myFile.replace(' >50K',1,inplace=True)
	myFile[0] = (myFile[0] - myFile[0].mean()) / (myFile[0].max() - myFile[0].min())
	df = myFile[0]
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[1]), prefix = 'workclass')], axis=1)#
	myFile[2] = (myFile[2] - myFile[2].mean()) / (myFile[2].max() - myFile[2].min())
	df = pd.concat([ df,myFile[2]], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[3]), prefix = 'education')], axis=1)
	myFile[4] = (myFile[4] - myFile[4].mean()) / (myFile[4].max() - myFile[4].min())
	df = pd.concat([ df,myFile[4]], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[5]), prefix = 'marital-status')], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[6]), prefix = 'occupation')], axis=1)#
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[7]), prefix = 'relationship')], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[8]), prefix = 'race')], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[9]),prefix = 'sex')], axis=1)
	myFile[10] = (myFile[10] - myFile[10].mean()) / (myFile[10].max() - myFile[10].min())
	myFile[11] = (myFile[11] - myFile[11].mean()) / (myFile[11].max() - myFile[11].min())
	myFile[12] = (myFile[12] - myFile[12].mean()) / (myFile[12].max() - myFile[12].min())
	df = pd.concat([ df,myFile[10]], axis=1)
	df = pd.concat([ df,myFile[11]], axis=1)
	df = pd.concat([ df,myFile[12]], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[13]), prefix = 'native-country')], axis=1)
	df = pd.concat([ df,pd.get_dummies(pd.Categorical(myFile[14]),prefix = 'output')], axis=1)
	return np.array(df)

train_data = clean('train_data')
val_data = clean('validation_data')
test_data = clean('test_data')

class Model(object):
	def __init__(self):
		super(Model, self).__init__()
		self.model = None

	def build_model(self): 
		self.model = Sequential()
		self.model.add(Dense(256, input_dim=105,activation='relu'))
		#self.model.add(Dense(256, activation='relu'))
		self.model.add(Dense(2, activation='softmax'))
		self.model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])

	def load_model(self, model_name, weights_name):
		json_file = open(model_name, 'r')
		loaded_model_json = json_file.read()
		json_file.close()
		self.model = model_from_json(loaded_model_json)
		self.model.load_weights(weights_name)
		print("Loaded model from disk")

	def save_model(self, model_name, weights_name):
		model_json = self.model.to_json()
		with open(model_name, "w") as json_file:
			json_file.write(model_json)
		self.model.save_weights(weights_name)
		print("Saved model to disk")

	def train_model(self,epochs,batch_size):
		train_data = clean('train_data')
		val_data = clean('validation_data')
		test_data = clean('test_data')
		X = train_data[:,0:-2]
		Y = train_data[:,-2:]
		dev_X = val_data[:,0:-2]
		dev_Y = val_data[:,-2:]
		return self.model.fit(np.array(X), np.array(Y),
					batch_size=batch_size,
					epochs=epochs,
					verbose=1,
					validation_data=(np.array(dev_X), np.array(dev_Y)))

epochs = 50
batch_size = 20
model_name = 'model.json'
weights_name = "model.h5"
census = Model()
census.build_model()
graph = census.train_model(epochs,batch_size)
#census.save_model(model_name, weights_name)

history = graph

# Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

predict = census.model.predict(test_data[:,0:-2])
predict = (np.shape(np.argmax(predict,axis=1)))
np.save('test_predict',predict)